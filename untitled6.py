# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13eJyie9Wh-aT8x9pIAmVi1gwnNKqklyY
"""

!pip install wooldridge
import wooldridge as wd
hprice = wd.data('hprice1')

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.metrics import mean_squared_error

hprice.info()

!pip install scikit-learn catboost

hprice = hprice[['price', 'assess', 'bdrms','lotsize','sqrft','colonial']]

hprice.info()

y = hprice['price']
X = hprice[['assess', 'bdrms','lotsize','sqrft','colonial']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression

model = LinearRegression()

param_grid = {
    'fit_intercept': [True, False],
    'normalize': [True, False]
}

from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import numpy as np

param_dist = {
    'fit_intercept': [True, False],
    'positive': [True, False]
}

model = LinearRegression()

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_

best_model = random_search.best_estimator_

mse = mean_squared_error(y_test, best_model.predict(X_test))
rmse = np.sqrt(mse)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import numpy as np

param_dist = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3],
}

model = GradientBoostingRegressor()

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_

best_model1 = random_search.best_estimator_

random_search.best_score_

mse = mean_squared_error(y_test, best_model.predict(X_test))
rmse = np.sqrt(mse)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import numpy as np

param_dist = {
    'max_iter': [80, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_leaf': [1, 2, 3],
    'l2_regularization': [0.0, 0.1, 0.2]
}

model = HistGradientBoostingRegressor()

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_

best_model2 = random_search.best_estimator_

random_search.best_score_

mse = mean_squared_error(y_test, best_model.predict(X_test))
rmse = np.sqrt(mse)

pip install xgboost

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import numpy as np

param_dist = {
    'n_estimators': [20, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_child_weight': [1, 2, 3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}
model = XGBRegressor()

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_

best_model3 = random_search.best_estimator_

random_search.best_score_

mse = mean_squared_error(y_test, best_model.predict(X_test))
rmse = np.sqrt(mse)

from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.datasets import make_classification
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd

from lightgbm.sklearn import LGBMRegressor
param_dist = {
    'n_estimators': [20, 50, 100],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_child_weight': [1, 2, 3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}
model = LGBMRegressor()

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)

random_search.fit(X_train, y_train)

random_search.best_score_

from catboost.core import CatBoostRegressor
param_dist = {
    'n_estimators': [20, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.7, 0.8, 0.9],
}
model = CatBoostRegressor()
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)
random_search.fit(X_train, y_train)

random_search.best_score_

import pickle
with open('modelo_entrenado.pkl', 'wb') as f:
    pickle.dump(best_model1, f)

